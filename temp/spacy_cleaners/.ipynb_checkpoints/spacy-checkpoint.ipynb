{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple 0 5 ORG\n",
      "U.K. 27 31 GPE\n",
      "$1 billion 44 54 MONEY\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en')\n",
    "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog dog 1.0\n",
      "dog cat 0.53906965\n",
      "dog banana 0.28761008\n",
      "cat dog 0.53906965\n",
      "cat cat 1.0\n",
      "cat banana 0.48752153\n",
      "banana dog 0.28761008\n",
      "banana cat 0.48752153\n",
      "banana banana 1.0\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en')\n",
    "tokens = nlp(u'dog cat banana')\n",
    "\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 4690420944186131903 X I I True False True en\n",
      "love 3702023516439754181 xxxx l ove True False False en\n",
      "coffee 3197928453018144401 xxxx c fee True False False en\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'I love coffee')\n",
    "for word in doc:\n",
    "    lexeme = doc.vocab[word.text]\n",
    "    print(lexeme.text, lexeme.orth, lexeme.shape_, lexeme.prefix_, lexeme.suffix_,\n",
    "          lexeme.is_alpha, lexeme.is_digit, lexeme.is_title, lexeme.lang_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Here', 'are', 'two', 'sentences', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'Hello, world. Here are two sentences.')\n",
    "print([t.text for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-grained POS tag PROPN 95\n",
      "Coarse-grained POS tag NNP 15794550382381185553\n",
      "Word shape Xxxxx 16072095006890171862\n",
      "Alphanumeric characters? True\n",
      "Punctuation mark? False\n",
      "Digit? False\n",
      "Like a number? True\n",
      "Like an email address? False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "apple = doc[0]\n",
    "print('Fine-grained POS tag', apple.pos_, apple.pos)\n",
    "print('Coarse-grained POS tag', apple.tag_, apple.tag)\n",
    "print('Word shape', apple.shape_, apple.shape)\n",
    "print('Alphanumeric characters?', apple.is_alpha)\n",
    "print('Punctuation mark?', apple.is_punct)\n",
    "\n",
    "billion = doc[10]\n",
    "print('Digit?', billion.is_digit)\n",
    "print('Like a number?', billion.like_num)\n",
    "print('Like an email address?', billion.like_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "San Francisco 0 13 GPE\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'San Francisco considers banning sidewalk delivery robots')\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[93m    Serving on port 5000...\u001b[0m\n",
      "    Using the 'dep' visualizer\n",
      "\n",
      "\n",
      "    Shutting down server on port 5000.\n",
      "\n",
      "\n",
      "\u001b[93m    Serving on port 5000...\u001b[0m\n",
      "    Using the 'ent' visualizer\n",
      "\n",
      "\n",
      "    Shutting down server on port 5000.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "\n",
    "doc_dep = nlp(u'This is a sentence.')\n",
    "displacy.serve(doc_dep, style='dep')\n",
    "\n",
    "doc_ent = nlp(u'When Sebastian Thrun started working on self-driving cars at Google '\n",
    "              u'in 2007, few people outside of the company took him seriously.')\n",
    "displacy.serve(doc_ent, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple <-> banana 0.37047377\n",
      "pasta <-> hippo 0.40645224\n",
      "True True True True\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(u\"Apple and banana are similar. Pasta and hippo aren't.\")\n",
    "\n",
    "apple = doc[0]\n",
    "banana = doc[2]\n",
    "pasta = doc[6]\n",
    "hippo = doc[8]\n",
    "\n",
    "print('apple <-> banana', apple.similarity(banana))\n",
    "print('pasta <-> hippo', pasta.similarity(hippo))\n",
    "print(apple.has_vector, banana.has_vector, pasta.has_vector, hippo.has_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['advmod', 'advcl', 'compound', 'nsubj', 'advcl', 'nsubj', 'advcl', 'advcl', 'xcomp', 'advcl', 'prep', 'xcomp', 'advcl', 'npadvmod', 'amod', 'pobj', 'prep', 'xcomp', 'advcl', 'punct', 'amod', 'pobj', 'prep', 'xcomp', 'advcl', 'amod', 'pobj', 'prep', 'xcomp', 'advcl', 'pobj', 'prep', 'xcomp', 'advcl', 'prep', 'xcomp', 'advcl', 'pobj', 'prep', 'xcomp', 'advcl', 'prep', 'xcomp', 'advcl', 'pobj', 'prep', 'xcomp', 'advcl', 'punct', 'amod', 'nsubj', 'nsubj', 'prep', 'nsubj', 'prep', 'prep', 'nsubj', 'det', 'pobj', 'prep', 'prep', 'nsubj', 'pobj', 'prep', 'prep', 'nsubj', 'dobj', 'advmod', 'punct']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u\"When Sebastian Thrun started working on self-driving cars at Google \"\n",
    "          u\"in 2007, few people outside of the company took him seriously.\")\n",
    "\n",
    "dep_labels = []\n",
    "for token in doc:\n",
    "    while token.head != token:\n",
    "        dep_labels.append(token.dep_)\n",
    "        token = token.head\n",
    "print(dep_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous cars cars nsubj shift\n",
      "insurance liability liability dobj shift\n",
      "manufacturers manufacturers pobj toward\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u\"Autonomous cars shift insurance liability toward manufacturers\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
    "          chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple apple PROPN NNP nsubj Xxxxx True False\n",
      "is be VERB VBZ aux xx True True\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True True\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "U.K. u.k. PROPN NNP compound X.X. False False\n",
      "startup startup NOUN NN dobj xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "          token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autonomous cars shift insurance liability toward manufacturers"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous autonomous\n",
      "cars car\n",
      "shift shift\n",
      "insurance insurance\n",
      "liability liability\n",
      "toward toward\n",
      "manufacturers manufacturer\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I -PRON- PRON PRP nsubj\n",
      "love love VERB VBP ROOT\n",
      "my -PRON- ADJ PRP$ poss\n",
      "love love NOUN NN dobj\n",
      "but but CCONJ CC cc\n",
      "my -PRON- ADJ PRP$ poss\n",
      "love love NOUN NN nsubj\n",
      "does do VERB VBZ aux\n",
      "n't not ADV RB neg\n",
      "love love VERB VB conj\n",
      "me -PRON- PRON PRP dobj\n",
      "as as ADP IN mark\n",
      "I -PRON- PRON PRP nsubj\n",
      "love love VERB VBP advcl\n",
      "my -PRON- ADJ PRP$ poss\n",
      "love love NOUN NN dobj\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'I love my love but my love doesn\\'t love me as I love my love')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x92 in position 16: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-3cb1246164bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'AI_peopleschina.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, float_precision, nrows, iterator, chunksize, verbose, encoding, squeeze, mangle_dupe_cols, tupleize_cols, infer_datetime_format, skip_blank_lines)\u001b[0m\n\u001b[1;32m    496\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m _parser_defaults = {\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    745\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skip_footer not supported for iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'as_recarray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.read (pandas/parser.c:7988)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._read_low_memory (pandas/parser.c:8244)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._read_rows (pandas/parser.c:9261)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._convert_column_data (pandas/parser.c:10654)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._convert_tokens (pandas/parser.c:11540)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._convert_with_dtype (pandas/parser.c:12976)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._string_convert (pandas/parser.c:13222)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser._string_box_utf8 (pandas/parser.c:18598)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x92 in position 16: invalid start byte"
     ]
    }
   ],
   "source": [
    "import codecs, csv\n",
    "import pandas as pd\n",
    "dat = pd.read_csv('AI_peopleschina.csv')\n",
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs, csv\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "nlp = spacy.load('en')\n",
    "STOP_WORDS = list(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = []\n",
    "counter = 1\n",
    "with open('AI_peopleschina.csv') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        news.append(row)\n",
    "        if counter > 5: break\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news[0]['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 is processed\n",
      "Document 2 is processed\n",
      "Document 3 is processed\n",
      "Document 4 is processed\n",
      "Document 5 is processed\n",
      "Document 6 is processed\n"
     ]
    }
   ],
   "source": [
    "counter = 1\n",
    "poss = ['NOUN','ADJ','ADV']\n",
    "tags = []\n",
    "lemmatize = True\n",
    "outnews = []\n",
    "poss.append('PUNCT')\n",
    "\n",
    "for new in news:\n",
    "    doc = nlp(new['Text'])\n",
    "    filtered_words  = []\n",
    "    for token in doc:\n",
    "        if (token.pos_ in poss or token.tag_ in tags) and token.lemma_ not in STOP_WORDS + ['-PRON-']:\n",
    "            outword = token.lemma_ if lemmatize else token.text\n",
    "            filtered_words.append(outword)\n",
    "    print('Document %s is processed' % counter)\n",
    "    new['Text'] = ' '.join(filtered_words)\n",
    "    outnews.append(new)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'machine stake decade , people rise advanced computing robotic technology life . , warning robot human economy , livelihood , especially low - skill worker . forward vast economic opportunity robot , , example , productivity undesirable job . venture capitalist , recently debate , camp , robot future high price low wage . right require , foremost , understanding way human historically value : leg , finger , mouth , brain , smile mind . leg large muscle thing , finger useful pattern . brain routine activity , leg- finger - work track . mouth - , word , spoken - . smile , roughly direction . finally , mind - curiosity creativity - important interesting challenge . , , argument - robot doomsayer - impact artificial intelligence advanced robotic labor force globalization impact advanced - country worker . globalization low - skill worker place , people faraway country leg - - finger position global division labor . new competitor low wage , obvious choice company . , key difference phenomenon rise robot consumption . - country worker advantage bargaining power globalization resource consumption . computer robot , contrast , electricity , leg , finger , brain activity faster efficiently human . , example experience ceo . instead human item batch transaction indication fraud , computer obviously legitimate transaction , fraudulent thoughtful consideration human . worker computer system worker generation ago . computer system thing food , - fold increase productivity entirely benefit middle class . way , globalization wage low - skill advanced - country worker job cheaply , value . computer high - skill worker - low - skill worker large robotic factory warehouse - time valuable activity , computer little . argument correct . far airtight . fact , old diamond - - water paradox - water essential , , diamond virtually useless , extremely expensive - albeit sophisticated subtle way . paradox , market economy , value water total usefulness water ( infinite ) average usefulness water ( large ) , marginal value drop water ( low ) . similarly , wage salary low- high - skill worker robot - computer economy future ( high ) productivity low - skill worker robot place high - skill worker software . instead , compensation worker highly productive computer - robot economy . newly city high level labor productivity world . factory worker wage extraordinary productivity , potato field pre - famine . question robot computer human labor good , high - tech service , information - sector infinitely productive . . job outside robot - computer economy - job people mouth , smile , mind - valuable high demand . , rapid technological progress wage increase line productivity gain . protracted process income - distribution equalization , machine , human leg , finger job machine - minding , human brain mouth , sector routine muscle power dexterity work . real income leisure time , demand smile product mind . occur machine routine brainwork ? maybe . far safe bet entire argument , . ( late news , @peoplesdaily http://www.facebook.com/peoplesdaily )'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outnews[0]['Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpacyCleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs, csv\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "nlp = spacy.load('en')\n",
    "STOP_WORDS = list(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = []\n",
    "counter = 1\n",
    "with open('AI_peopleschina.csv') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        news.append(row)\n",
    "        if counter > 5: break\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_cleaner(news_list,poss = ['NOUN','ADJ','ADV'],tags = [],lemmatize = True, cleansws = True, filterpunct=True):\n",
    "    outnews = []\n",
    "    if not filterpunct: poss.append('PUNCT')\n",
    "    counter = 1\n",
    "    sws = STOP_WORDS + ['-PRON-'] if cleansws else ['-PRON-']\n",
    "    for new in news:\n",
    "        doc = nlp(new['Text'])\n",
    "        filtered_words  = []\n",
    "        for token in doc:\n",
    "            if (token.pos_ in poss or token.tag_ in tags) and token.lemma_ not in STOP_WORDS + ['-PRON-']:\n",
    "                outword = token.lemma_ if lemmatize else token.text\n",
    "                filtered_words.append(outword)\n",
    "        print('Document %s is processed' % counter)\n",
    "        new['Text'] = ' '.join(filtered_words)\n",
    "        outnews.append(new)\n",
    "        counter += 1\n",
    "    return(outnews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## csv_cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_cleaner(csvdest,poss = ['NOUN','ADJ','ADV'],tags = [],lemmatize = True, cleansws = True, filterpunct=True):\n",
    "    # initializing\n",
    "    csvname = csvdest.replace('.csv','')\n",
    "    if not filterpunct: poss.append('PUNCT')\n",
    "    sws = STOP_WORDS + ['-PRON-'] if cleansws else ['-PRON-']\n",
    "    counter = 1\n",
    "    # opening csv files, both input and output\n",
    "    writefile = open(csvname + '_processed.csv','w') # output file\n",
    "    with open(csvdest) as readfile:\n",
    "        reader = csv.DictReader(readfile)\n",
    "        writer = csv.DictWriter(writefile, fieldnames=reader.fieldnames)\n",
    "        writer.writeheader()\n",
    "        # reading, processing and writing files\n",
    "        for news in reader:\n",
    "            doc = nlp(news['Text'])\n",
    "            filtered_words  = []\n",
    "            filtered_lemmas = []\n",
    "            for token in doc: # select words if conditions hold\n",
    "                if (token.pos_ in poss or token.tag_ in tags) and token.lemma_ not in STOP_WORDS + ['-PRON-']:\n",
    "                    filtered_words.append(token.text)\n",
    "                    filtered_lemmas.append(token.lemma_)\n",
    "            print('Document %s is processed' % counter)\n",
    "            news['Text'] = ' '.join(filtered_lemmas) if lemmatize else ' '.join(filtered_words)\n",
    "            writer.writerow(news)\n",
    "            counter += 1\n",
    "    writefile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 is processed\n",
      "Document 2 is processed\n",
      "Document 3 is processed\n",
      "Document 4 is processed\n",
      "Document 5 is processed\n"
     ]
    }
   ],
   "source": [
    "csv_cleaner('AI_peopleschina.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "\n",
    "\n",
    "def json_cleaner(jsondest,poss = ['NOUN','ADJ','ADV'],tags = [],lemmatize = True, cleansws = True, filterpunct=True):\n",
    "    # initializing\n",
    "    jsonname = jsondest.replace('.json','')\n",
    "    if not filterpunct: poss.append('PUNCT')\n",
    "    sws = STOP_WORDS + ['-PRON-'] if cleansws else ['-PRON-']\n",
    "    counter = 1\n",
    "    # opening json files, both input and output\n",
    "    writefile = open(jsonname + '_processed.json','w') # output file\n",
    "    writefile.write('[')\n",
    "    with open(jsondest) as readfile:\n",
    "        for news in readfile:\n",
    "            if len(news) < 10: continue\n",
    "            news = news.strip()\n",
    "            if news.endswith(','): news = news[:-1]\n",
    "            news = json.loads(news)\n",
    "            doc = nlp(news['Text'])\n",
    "            filtered_words  = []\n",
    "            filtered_lemmas = []\n",
    "            for token in doc: # select words if conditions hold\n",
    "                if (token.pos_ in poss or token.tag_ in tags) and token.lemma_ not in STOP_WORDS + ['-PRON-']:\n",
    "                    filtered_words.append(token.text)\n",
    "                    filtered_lemmas.append(token.lemma_)\n",
    "            print('Document %s is processed' % counter)\n",
    "            news['Text'] = ' '.join(filtered_lemmas) if lemmatize else ' '.join(filtered_words)\n",
    "            # Writing\n",
    "            writefile.write('\\n')\n",
    "            json.dump(news, writefile)\n",
    "            writefile.write(',')\n",
    "            counter += 1\n",
    "    # Son iterasyonda eklenen gereksiz virgul'u sil\n",
    "    writefile.seek(writefile.tell()-1)\n",
    "    writefile.truncate()\n",
    "    # Bir satir asagiya\n",
    "    writefile.write('\\n]')\n",
    "    writefile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply on test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 is processed\n",
      "Document 2 is processed\n",
      "Document 3 is processed\n",
      "Document 4 is processed\n",
      "Document 5 is processed\n",
      "Document 6 is processed\n"
     ]
    }
   ],
   "source": [
    "csv_cleaner('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 is processed\n",
      "Document 2 is processed\n",
      "Document 3 is processed\n",
      "Document 4 is processed\n"
     ]
    }
   ],
   "source": [
    "json_cleaner('test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>source</th>\n",
       "      <th>Date</th>\n",
       "      <th>link</th>\n",
       "      <th>Text</th>\n",
       "      <th>queryword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>peopleschina</td>\n",
       "      <td>00:00.0</td>\n",
       "      <td>http://en.people.cn/n/2014/1126/c202936-881422...</td>\n",
       "      <td>machine stake decade people rise advanced comp...</td>\n",
       "      <td>'% artificial intelligence%'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>peopleschina</td>\n",
       "      <td>00:00.0</td>\n",
       "      <td>http://en.people.cn/n3/2017/0821/c202936-92580...</td>\n",
       "      <td>photo artificial intelligence treatment center...</td>\n",
       "      <td>'% artificial intelligence%'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>peopleschina</td>\n",
       "      <td>00:00.0</td>\n",
       "      <td>http://en.people.cn/business/n3/2017/0817/c907...</td>\n",
       "      <td>second left executive director chairman execut...</td>\n",
       "      <td>'% artificial intelligence%'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>peopleschina</td>\n",
       "      <td>00:00.0</td>\n",
       "      <td>http://en.people.cn/business/n3/2017/0810/c907...</td>\n",
       "      <td>telecom service provider high profit growth ha...</td>\n",
       "      <td>'% artificial intelligence%'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>peopleschina</td>\n",
       "      <td>00:00.0</td>\n",
       "      <td>http://en.people.cn/n3/2017/0805/c90000-925136...</td>\n",
       "      <td>scanner traditional chinese painting time big ...</td>\n",
       "      <td>'% artificial intelligence%'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>peopleschina</td>\n",
       "      <td>00:00.0</td>\n",
       "      <td>http://en.people.cn/n3/2017/0527/c90000-922154...</td>\n",
       "      <td>world weiqi player contest artificial intellig...</td>\n",
       "      <td>'% artificial intelligence%'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        source     Date  \\\n",
       "0           0  peopleschina  00:00.0   \n",
       "1           1  peopleschina  00:00.0   \n",
       "2           2  peopleschina  00:00.0   \n",
       "3           3  peopleschina  00:00.0   \n",
       "4           4  peopleschina  00:00.0   \n",
       "5           5  peopleschina  00:00.0   \n",
       "\n",
       "                                                link  \\\n",
       "0  http://en.people.cn/n/2014/1126/c202936-881422...   \n",
       "1  http://en.people.cn/n3/2017/0821/c202936-92580...   \n",
       "2  http://en.people.cn/business/n3/2017/0817/c907...   \n",
       "3  http://en.people.cn/business/n3/2017/0810/c907...   \n",
       "4  http://en.people.cn/n3/2017/0805/c90000-925136...   \n",
       "5  http://en.people.cn/n3/2017/0527/c90000-922154...   \n",
       "\n",
       "                                                Text  \\\n",
       "0  machine stake decade people rise advanced comp...   \n",
       "1  photo artificial intelligence treatment center...   \n",
       "2  second left executive director chairman execut...   \n",
       "3  telecom service provider high profit growth ha...   \n",
       "4  scanner traditional chinese painting time big ...   \n",
       "5  world weiqi player contest artificial intellig...   \n",
       "\n",
       "                      queryword  \n",
       "0  '% artificial intelligence%'  \n",
       "1  '% artificial intelligence%'  \n",
       "2  '% artificial intelligence%'  \n",
       "3  '% artificial intelligence%'  \n",
       "4  '% artificial intelligence%'  \n",
       "5  '% artificial intelligence%'  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('test_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Text</th>\n",
       "      <th>link</th>\n",
       "      <th>queryword</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-06-05</td>\n",
       "      <td>machine stake decade people rise advanced comp...</td>\n",
       "      <td>http://en.people.cn/n/2014/1126/c202936-881422...</td>\n",
       "      <td>'% artificial intelligence%'</td>\n",
       "      <td>peopleschina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-06-05</td>\n",
       "      <td>photo artificial intelligence treatment center...</td>\n",
       "      <td>http://en.people.cn/n3/2017/0821/c202936-92580...</td>\n",
       "      <td>'% artificial intelligence%'</td>\n",
       "      <td>peopleschina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-06-05</td>\n",
       "      <td>second left executive director chairman execut...</td>\n",
       "      <td>http://en.people.cn/business/n3/2017/0817/c907...</td>\n",
       "      <td>'% artificial intelligence%'</td>\n",
       "      <td>peopleschina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2018-06-05</td>\n",
       "      <td>telecom service provider high profit growth ha...</td>\n",
       "      <td>http://en.people.cn/business/n3/2017/0810/c907...</td>\n",
       "      <td>'% artificial intelligence%'</td>\n",
       "      <td>peopleschina</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date                                               Text  \\\n",
       "0  0 2018-06-05  machine stake decade people rise advanced comp...   \n",
       "1  1 2018-06-05  photo artificial intelligence treatment center...   \n",
       "2  2 2018-06-05  second left executive director chairman execut...   \n",
       "3  3 2018-06-05  telecom service provider high profit growth ha...   \n",
       "\n",
       "                                                link  \\\n",
       "0  http://en.people.cn/n/2014/1126/c202936-881422...   \n",
       "1  http://en.people.cn/n3/2017/0821/c202936-92580...   \n",
       "2  http://en.people.cn/business/n3/2017/0817/c907...   \n",
       "3  http://en.people.cn/business/n3/2017/0810/c907...   \n",
       "\n",
       "                      queryword        source  \n",
       "0  '% artificial intelligence%'  peopleschina  \n",
       "1  '% artificial intelligence%'  peopleschina  \n",
       "2  '% artificial intelligence%'  peopleschina  \n",
       "3  '% artificial intelligence%'  peopleschina  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_json('test_processed.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
